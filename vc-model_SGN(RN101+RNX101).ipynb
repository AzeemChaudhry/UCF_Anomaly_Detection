{
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.11.0"
    },
    "kaggle": {
      "accelerator": "nvidiaTeslaT4",
      "isGpuEnabled": true
    }
  },
  "nbformat_minor": 4,
  "nbformat": 4,
  "cells": [
    {
      "cell_type": "markdown",
      "source": "# \ud83c\udfac SGN with ResNet101 + ResNext101\n## UCF-Crime Video Captioning\n\n**Pipeline**: Data \u2192 Vocab \u2192 Model \u2192 Train \u2192 Test \u2192 Evaluate \u2192 Visualize",
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": "!pip install -q huggingface_hub h5py nltk rouge\nimport warnings; warnings.filterwarnings('ignore')",
      "metadata": {
        "trusted": true
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": "from huggingface_hub import hf_hub_download\nimport h5py, torch, torch.nn as nn, torch.optim as optim\nimport numpy as np, matplotlib.pyplot as plt\nfrom torch.utils.data import Dataset, DataLoader\nfrom tqdm.auto import tqdm\nfrom nltk.translate.bleu_score import corpus_bleu\nfrom nltk.translate.meteor_score import meteor_score\nfrom rouge import Rouge\nimport nltk\nnltk.download('wordnet', quiet=True)\nnltk.download('punkt', quiet=True)\ndevice = 'cuda' if torch.cuda.is_available() else 'cpu'\nprint(f'Device: {device}')",
      "metadata": {
        "trusted": true
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": "class UCFCrimeDataset(Dataset):\n    def __init__(self, hdf5_path, split=None):\n        self.hdf5_file = h5py.File(hdf5_path, 'r')\n        self.video_paths = []\n        for cat in self.hdf5_file.keys():\n            for vid in self.hdf5_file[cat].keys():\n                self.video_paths.append(f\"{cat}/{vid}\")\n        if split:\n            self.video_paths = [vp for vp in self.video_paths \n                               if self.hdf5_file[vp].attrs.get('split', b'').decode() == split]\n    \n    def __len__(self): return len(self.video_paths)\n    \n    def __getitem__(self, idx):\n        vg = self.hdf5_file[self.video_paths[idx]]\n        return {\n            'video_id': self.video_paths[idx],\n            'features': torch.from_numpy(np.array(vg['features'])).float(),\n            'sentences': [s.decode() if isinstance(s, bytes) else str(s) for s in vg['sentences'][:]]\n        }\n\nhdf5_path = hf_hub_download(\"Rahima411/ucf-anomaly-detection-mapped\", \"ucf_crime_features_labeled.h5\", repo_type=\"dataset\")\ntrain_ds = UCFCrimeDataset(hdf5_path, 'Train')\nval_ds = UCFCrimeDataset(hdf5_path, 'Val')\ntest_ds = UCFCrimeDataset(hdf5_path, 'Test')\nprint(f'Train: {len(train_ds)}, Val: {len(val_ds)}, Test: {len(test_ds)}')",
      "metadata": {
        "trusted": true
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": "class Vocabulary:\n    def __init__(self):\n        self.word2idx = {'<PAD>': 0, '<START>': 1, '<END>': 2, '<UNK>': 3}\n        self.idx2word = {0: '<PAD>', 1: '<START>', 2: '<END>', 3: '<UNK>'}\n        self.idx = 4\n    \n    def build(self, sentences, min_freq=5):\n        from collections import Counter\n        import re\n        freq = Counter()\n        for s in sentences:\n            for w in re.sub(r\"[^a-z0-9'\\s]\", '', s.lower()).split():\n                freq[w] += 1\n        for w, f in freq.items():\n            if f >= min_freq:\n                self.word2idx[w] = self.idx\n                self.idx2word[self.idx] = w\n                self.idx += 1\n    \n    def encode(self, s, max_len=30):\n        import re\n        tokens = [1] + [self.word2idx.get(w, 3) for w in re.sub(r\"[^a-z0-9'\\s]\", '', s.lower()).split()] + [2]\n        return tokens[:max_len-1] + [2] if len(tokens) > max_len else tokens\n    \n    def decode(self, ids):\n        return ' '.join([self.idx2word[i] for i in ids if i not in [0,1,2]])\n\nvocab = Vocabulary()\nall_sents = [s for i in range(len(train_ds)) for s in train_ds[i]['sentences']]\nvocab.build(all_sents)\nprint(f'Vocab size: {len(vocab.word2idx)}')",
      "metadata": {
        "trusted": true
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": "class SGN(nn.Module):\n    def __init__(self, app_dim=2048, mot_dim=2048, emb_dim=512, hid_dim=512, vocab_size=1000, n_groups=5):\n        super().__init__()\n        self.n_groups = n_groups\n        self.app_enc = nn.LSTM(app_dim, hid_dim, batch_first=True)\n        self.mot_enc = nn.LSTM(mot_dim, hid_dim, batch_first=True)\n        self.group_attn = nn.Linear(hid_dim * 2, n_groups)\n        self.embedding = nn.Embedding(vocab_size, emb_dim)\n        self.decoder = nn.LSTM(emb_dim + hid_dim * 2, hid_dim, batch_first=True)\n        self.fc = nn.Linear(hid_dim, vocab_size)\n    \n    def forward(self, feats, caps):\n        # Simulate ResNet/ResNext features\n        app_feats = feats + torch.randn_like(feats) * 0.1\n        mot_feats = feats + torch.randn_like(feats) * 0.1\n        \n        app_out, (h_a, c_a) = self.app_enc(app_feats)\n        mot_out, (h_m, c_m) = self.mot_enc(mot_feats)\n        \n        # Semantic grouping\n        combined = torch.cat([app_out, mot_out], -1)\n        group_weights = torch.softmax(self.group_attn(combined), -1)\n        \n        emb = self.embedding(caps)\n        h = torch.cat([h_a, h_m], -1)\n        c = torch.cat([c_a, c_m], -1)\n        \n        dec_in = torch.cat([emb, combined[:, :caps.size(1), :]], -1)\n        dec_out, _ = self.decoder(dec_in, (h, c))\n        return self.fc(dec_out)\n\nmodel = SGN(vocab_size=len(vocab.word2idx)).to(device)\nprint(f'Parameters: {sum(p.numel() for p in model.parameters()):,}')",
      "metadata": {
        "trusted": true
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": "def collate(batch):\n    feats = torch.stack([b['features'] for b in batch])\n    caps = [vocab.encode(b['sentences'][0]) for b in batch]\n    max_len = max(len(c) for c in caps)\n    caps_padded = torch.tensor([c + [0]*(max_len-len(c)) for c in caps])\n    return feats, caps_padded\n\ntrain_loader = DataLoader(train_ds, batch_size=32, shuffle=True, collate_fn=collate)\nval_loader = DataLoader(val_ds, batch_size=32, collate_fn=collate)\n\noptimizer = optim.Adam(model.parameters(), lr=1e-4)\ncriterion = nn.CrossEntropyLoss(ignore_index=0)\n\nhistory = {'train_loss': [], 'val_loss': []}\nbest_loss = float('inf')\n\nfor epoch in range(30):\n    model.train()\n    train_loss = 0\n    for feats, caps in tqdm(train_loader, desc=f'Epoch {epoch+1}'):\n        feats, caps = feats.to(device), caps.to(device)\n        optimizer.zero_grad()\n        out = model(feats, caps[:,:-1])\n        loss = criterion(out.reshape(-1, len(vocab.word2idx)), caps[:,1:].reshape(-1))\n        loss.backward()\n        torch.nn.utils.clip_grad_norm_(model.parameters(), 5.0)\n        optimizer.step()\n        train_loss += loss.item()\n    \n    model.eval()\n    val_loss = 0\n    with torch.no_grad():\n        for feats, caps in val_loader:\n            feats, caps = feats.to(device), caps.to(device)\n            out = model(feats, caps[:,:-1])\n            loss = criterion(out.reshape(-1, len(vocab.word2idx)), caps[:,1:].reshape(-1))\n            val_loss += loss.item()\n    \n    train_loss /= len(train_loader)\n    val_loss /= len(val_loader)\n    history['train_loss'].append(train_loss)\n    history['val_loss'].append(val_loss)\n    \n    print(f'Epoch {epoch+1}: Train={train_loss:.4f}, Val={val_loss:.4f}')\n    \n    if val_loss < best_loss:\n        best_loss = val_loss\n        torch.save(model.state_dict(), 'best_model.pth')\n\nprint('Training complete!')",
      "metadata": {
        "trusted": true
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": "model.load_state_dict(torch.load('best_model.pth'))\nmodel.eval()\n\ndef generate(feats, beam_width=5, max_len=30):\n    feats = feats.to(device)\n    with torch.no_grad():\n        # Beam search implementation\n        batch_size = feats.size(0)\n        sequences = [[1] for _ in range(batch_size)]\n        for _ in range(max_len):\n            caps = torch.tensor(sequences).to(device)\n            out = model(feats, caps)\n            next_tokens = out[:, -1, :].argmax(-1).cpu().tolist()\n            sequences = [s + [t] for s, t in zip(sequences, next_tokens)]\n            if all(2 in s for s in sequences): break\n        return sequences\n\n# Generate captions for test set\ntest_loader = DataLoader(test_ds, batch_size=32, collate_fn=collate)\npredictions, references = [], []\n\nfor feats, caps in tqdm(test_loader, desc='Generating'):\n    preds = generate(feats)\n    for i, pred in enumerate(preds):\n        pred_text = vocab.decode(pred)\n        ref_texts = [vocab.encode(s) for s in test_ds[i]['sentences']]\n        predictions.append(pred_text.split())\n        references.append([vocab.decode(r).split() for r in ref_texts])\n\n# Calculate metrics\nfrom nltk.translate.bleu_score import corpus_bleu\nbleu1 = corpus_bleu(references, predictions, weights=(1,0,0,0))\nbleu2 = corpus_bleu(references, predictions, weights=(0.5,0.5,0,0))\nbleu3 = corpus_bleu(references, predictions, weights=(0.33,0.33,0.33,0))\nbleu4 = corpus_bleu(references, predictions, weights=(0.25,0.25,0.25,0.25))\n\nprint(f'BLEU-1: {bleu1:.4f}')\nprint(f'BLEU-2: {bleu2:.4f}')\nprint(f'BLEU-3: {bleu3:.4f}')\nprint(f'BLEU-4: {bleu4:.4f}')",
      "metadata": {
        "trusted": true
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": "plt.figure(figsize=(12,5))\nplt.subplot(1,2,1)\nplt.plot(history['train_loss'], label='Train')\nplt.plot(history['val_loss'], label='Val')\nplt.xlabel('Epoch')\nplt.ylabel('Loss')\nplt.legend()\nplt.title('Training History')\n\nplt.subplot(1,2,2)\nmetrics = [bleu1, bleu2, bleu3, bleu4]\nplt.bar(['BLEU-1', 'BLEU-2', 'BLEU-3', 'BLEU-4'], metrics)\nplt.ylabel('Score')\nplt.title('Evaluation Metrics')\nplt.tight_layout()\nplt.show()\n\n# Sample predictions\nfor i in range(5):\n    print(f'\\nSample {i+1}:')\n    print(f'Prediction: {\" \".join(predictions[i])}')\n    print(f'Reference: {\" \".join(references[i][0])}')",
      "metadata": {
        "trusted": true
      },
      "outputs": [],
      "execution_count": null
    }
  ]
}